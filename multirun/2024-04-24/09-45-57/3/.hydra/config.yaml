network:
  name: AttentionUNet
  device: cuda
  dtype: float32
  components:
    patch_size: None
    hidden_d: 48
    sequence_lenght: 50
    decoder: UNetenc UNetbase MHABatchFeature Unetdec
    activation: GMMActivationV2
optimizer:
  name: Adam
  params:
    lr: 0.001
    amsgrad: true
training:
  name: AttentionUNet
  iterations: 150
dataset:
  name: lab_logo_dense4
  offset: 0
  batch_size: 50
