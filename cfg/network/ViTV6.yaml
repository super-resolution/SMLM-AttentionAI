name: ViTV6
device: cuda
#implements a deeper UNet in comparison to V6
#used datatype use float 16!
#batch size is 250
#using larger training dataset
#doubles hidden_d
dtype: float16
components:
  #patch size of ViT has to be quadratic
  patch_size: None
  #hidden dimension worked best out of 400,800,1600,3200
  hidden_d: 48*4
  #define sequence length for positional encoding
  sequence_lenght: 50
  embedding:
    UNet
  #define decoder architecture
  decoder:
    UNetenc
    UNetbase
    MHABatchFeature
    MHAXYFeature
    Unetdec
  #currently linear and conv
  #define activation
  activation:
    GMMActivationV2
  #currently GMM activation