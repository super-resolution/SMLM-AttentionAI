name: AttentionUNet
device: cuda
#used datatype todo: use float 16?
#batch size is 250
#using larger training dataset
#doubles hidden_d
dtype: float32
components:
  #patch size of ViT has to be quadratic
  patch_size: None
  #hidden dimension worked best out of 400,800,1600,3200
  hidden_d: 48
  #define sequence length for positional encoding
  sequence_lenght: 50
  #define feature-extractor unet in different file
  decoder:
    UNetenc
    UNetbase
    MHABatchFeature
    Unetdec
  #currently linear and conv
  #define activation
  activation:
    GMMActivationV2
  #currently GMM activation